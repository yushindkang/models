{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8cbd7ca-bb49-4396-8dad-c32e3ad1ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image as PI\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "598e13d2-0c5e-4b23-bc0e-07a0cd5691e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "\n",
    "    image_names = []\n",
    "    folders = glob.glob(\"../Training/*\")\n",
    "#    for folder in folders:\n",
    "    for i in range(1):\n",
    "        for file in glob.glob(folders[i] + '/*.jpg'):\n",
    "            image_names.append(file)\n",
    "            with open(os.path.join(file), 'rb') as i:\n",
    "                img = PI.open(i)\n",
    "                img_sequence = img.getdata()\n",
    "                img_array = np.array(img_sequence)\n",
    "                train_x.append(img_array)\n",
    "\n",
    "            dir_path = os.path.dirname(file)\n",
    "            folder_name = os.path.basename(dir_path)\n",
    "            train_y.append(folder_name)\n",
    "\n",
    "    return np.array(train_x), np.array([train_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dfd5c41-d52b-4635-9771-9c5aa59abbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters_deep(layer_dims):\n",
    "    np.random.seed(3)\n",
    "\n",
    "    parameters = {}\n",
    "\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "084dd360-3d22-4021-9868-628b61f5edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear part of the layer's forward propagation.\n",
    "def linear_forward(A, W, b):\n",
    "\n",
    "    Z = np.dot(W, A) + b\n",
    "\n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4775126b-0de7-43b2-960f-5dd0088556cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57e8e7bf-b6ec-497e-b243-0ed56a27f941",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):   \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b299f4cc-d539-40af-bfa7-c382e81f1f51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relu(Z): \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "        \n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e508da3c-5d0d-4976-bc22-8fa6dbbee89f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2d34ac3-0085-45e5-acff-e8ed117f6fa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    \n",
    "    dZ = np.array(dA, copy=True) # Set dz to correct object.\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec417b53-3f29-429a-91ac-d2dff972acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "#     print(\"Inside l model forward, L: \", L)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        \n",
    "        W = parameters[\"W\" + str(l)]\n",
    "        b = parameters[\"b\" + str(l)]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    \n",
    "    W = parameters[\"W\" + str(L)]\n",
    "    b = parameters[\"b\" + str(L)]\n",
    "    AL, cache = linear_activation_forward(A, W, b, \"sigmoid\")\n",
    "    \n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abe91484-f8f6-4680-b46a-59d535325101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    logprobs = np.multiply(Y, np.log(AL)) + (1 - Y) * (np.log(1 - AL))\n",
    "    \n",
    "    cost = - np.sum(logprobs)/m\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "543ee98d-b3da-40c2-962a-8bad8fe29b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    \n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True) # sum by the rows of dZ with keepdims=True\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9564119b-8d27-4a2c-9e56-3ae040c52165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb080a0f-1aaf-41c3-a449-663e696e7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    \n",
    "    L = len(caches)\n",
    "#     print(\"L model backward, L: \", L)\n",
    "    \n",
    "    m = AL.shape[1]\n",
    "#     print(\"AL shape: \", AL.shape)\n",
    "#     print(\"L model backward, m: \", m)\n",
    "    \n",
    "#     print(\"L model backward, Y before reshape: \", Y.shape)\n",
    "    Y = Y.reshape(AL.shape)\n",
    "#     print(\"L model backward, Y after reshape: \", Y.shape)\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "          \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dA_prev_temp, current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eab68e19-d33d-4986-8b37-f9957fe38f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    parameters = params.copy()\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    \n",
    "    return parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0287e4ab-fe85-4264-a4eb-18284b92f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations, print_cost=False):\n",
    "    parameters = init_parameters_deep(layers_dims)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "                \n",
    "        cost = compute_cost(AL, Y)\n",
    "         \n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    # Plot the cost.\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bb1ad90-b0bc-4a72-bb03-736e716e4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_int_mapping_y(train_y):\n",
    "    \n",
    "    # Get list of all types of fruit/veg.\n",
    "    folders = glob.glob(\"../Training/*\")\n",
    "    words_y = []\n",
    "    \n",
    "    for folder in folders:\n",
    "        dir_path = os.path.basename(folder)\n",
    "        words_y.append(dir_path)\n",
    "    \n",
    "    words_y.sort()\n",
    "    \n",
    "    # Get num training examples and num of possible answers.\n",
    "    m = train_y.shape[1]\n",
    "    num_possible_answers = len(words_y)\n",
    "    \n",
    "    # Create a word to int dict for every type of fruit/veg.\n",
    "    word_to_int_y = {}\n",
    "    for i in range(num_possible_answers):\n",
    "        key = \"[\\'\" + words_y[i] +    \"\\']\"\n",
    "        word_to_int_y[key] = i\n",
    "        \n",
    "        \n",
    "    # Create a list of ints mapped from fruits and veg. Size is equal to m.\n",
    "    new_int_y = []\n",
    "    for word in train_y.T:\n",
    "        key = np.array2string(word)\n",
    "        int_y = word_to_int_y[key]\n",
    "        new_int_y.append(int_y)\n",
    "    \n",
    "    # Initliaise array of zeros with dims: possible_answers x m\n",
    "    final_init_y = np.zeros((num_possible_answers, m))\n",
    "    print(\"zeros shape: \", final_init_y.shape)\n",
    "    \n",
    "    # Initalise \"1\" for the \"ith\" position in each training example.\n",
    "    # The position of the \"1\" represents the fruit or vegetable.\n",
    "    i = 0\n",
    "    for single_zero_array, y_int_mapping in zip(final_init_y.T, new_int_y):\n",
    "        single_zero_array[y_int_mapping] = 1\n",
    "    \n",
    "    \n",
    "#     (unique, count) = np.unique(zeros, return_counts=True)\n",
    "    \n",
    "#     print(\"unique: \", unique)\n",
    "#     print(\"count: \", count)\n",
    "    \n",
    "    \n",
    "    return final_init_y\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7de48374-b8d5-406e-a193-3c45962815ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data...\n",
      "train_x original:  (479, 10000, 3)\n",
      "train_y original:  (1, 479)\n",
      "train_x after flatten:  (30000, 479)\n",
      "zeros shape:  (131, 479)\n",
      "Y shape after int init:  (131, 479)\n",
      "# layers L:  3\n",
      "# training examples:  479\n",
      "# output values:  131\n",
      "Running model...\n",
      "Cost after iteration 0: 90.800148\n",
      "Cost after iteration 100: 0.006619\n",
      "Cost after iteration 200: 0.002772\n",
      "Cost after iteration 300: 0.001703\n",
      "Cost after iteration 400: 0.001212\n",
      "Cost after iteration 500: 0.000934\n",
      "Cost after iteration 600: 0.000756\n",
      "Cost after iteration 700: 0.000633\n",
      "Cost after iteration 800: 0.000543\n",
      "Cost after iteration 900: 0.000474\n",
      "Cost after iteration 1000: 0.000421\n",
      "Cost after iteration 1100: 0.000377\n",
      "Cost after iteration 1200: 0.000342\n",
      "Cost after iteration 1300: 0.000312\n",
      "Cost after iteration 1400: 0.000287\n",
      "Cost after iteration 1500: 0.000265\n",
      "Cost after iteration 1600: 0.000247\n",
      "Cost after iteration 1700: 0.000230\n",
      "Cost after iteration 1800: 0.000216\n",
      "Cost after iteration 1900: 0.000203\n",
      "Cost after iteration 2000: 0.000192\n",
      "Cost after iteration 2100: 0.000182\n",
      "Cost after iteration 2200: 0.000172\n",
      "Cost after iteration 2300: 0.000164\n",
      "Cost after iteration 2400: 0.000156\n",
      "Cost after iteration 2500: 0.000149\n",
      "Cost after iteration 2600: 0.000143\n",
      "Cost after iteration 2700: 0.000137\n",
      "Cost after iteration 2800: 0.000131\n",
      "Cost after iteration 2900: 0.000126\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model complete...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load training data.\n",
    "    train_x, train_y = load_train_data()\n",
    "    print(\"loading training data...\")\n",
    "    print(\"train_x original: \", train_x.shape)\n",
    "    print(\"train_y original: \", train_y.shape)\n",
    "\n",
    "    # Flatten train_x (every col is a training example) and normalise values to a range between 0 and 1.\n",
    "    train_x_flatten = train_x.reshape(train_x.shape[0], -1).T\n",
    "    train_x = train_x_flatten / 255\n",
    "    print(\"train_x after flatten: \", train_x_flatten.shape)\n",
    "    \n",
    "    # Map train_y to array of integer values\n",
    "    train_int_y = init_int_mapping_y(train_y)\n",
    "    print(\"Y shape after int init: \", train_int_y.shape)\n",
    "\n",
    "    # 3 layer network. Input layer = 100 pixels x 100 pixels x 3 rgb. Output layer = 131 = # possible fruits/veg.\n",
    "    layer_dims = [30000, 20, 20, 131]\n",
    "    parameters = init_parameters_deep(layer_dims)\n",
    "    print(\"# layers L: \", len(parameters)//2)\n",
    "#     for k, v in parameters.items():\n",
    "#         print(\"key: \", k)\n",
    "#         print(\"value shape: \", v.shape)\n",
    "    \n",
    "    print(\"# training examples: \", train_x.shape[1])\n",
    "    print(\"# output values: \", train_int_y.shape[0])\n",
    "        \n",
    "    print(\"Running model...\")\n",
    "    # L_layer_model(X, Y, layers_dims, learning_rate, num_iterations, print_cost=False)\n",
    "    parameters, costs = L_layer_model(train_x, train_int_y, layer_dims, 0.0075, num_iterations=3000, print_cost=True)\n",
    "    print(\"Model complete...\")\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da187288-5b60-447d-b0f4-31e687ca1a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dda209-39b9-418b-8f88-6c6204468424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779e65fe-4833-46b0-bce8-aca5ff270937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
